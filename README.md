# CNN_Image_Prediction
Implementing a Convolutional Neural Network to predict an image dataset and comparing the results with traditional MLP Neural Networks.

The Kuzushiji dataset that is used in the present Python program is a dataset that contains 60000 training images and 10000 testing images in grayscale (one channel) and of size 28x28. Kuzushiji comes in MNIST original format (packed byte-encoded images). The train dataset is not provided here because of its huge size, but it might be downloaded from Internet. There is implemented a pre-processing part consisting of checking the dataset structure and converting it in a suitable format for ingestion by the Neural Network.

There are also different Neural Network Architectures to deal with such a dataset, starting by the well-known Lenet Architecture and improving it by adding more complexity and regularization in a reasonable manner so that it leads to Accuracy improvements overall. The whole rationale of the choice of Architectures is not provided here, but available if necessary.

Finally, there is also an implementation of carefully designed Multi-Layer Perceptron Neural Networks to deal with the present image prediction problem, as well as other statistical techniques, such as Random Forests, to tackle the same prediction problem.


**The theoretical rationale of the final results and comparison among all used techniques for image prediction is provided here:**

Even though with the **Multi-Layer Perceptron Neural Network** we have more layers (up to 7 Fully-Connected layers) with many Neurons and we use ‘Dropout’ layers to be able to generalize better, with much more parameters than the LeNet CNN, we  don’t get a higher accuracy than LeNet. The main reason about such differences stems from the approach that the MLP and the CNN use to predict. In case of the MLP, it treats every pixel in the same way throughout all the images the Neural Network sees. Therefore, such approach is penalized when the  character is not written using the same pixels in every image and it yields to some noise to the Neural Network, so it can hardly infer some pattern in the same pixel over most of the images. In case of the **CNN approach**, it uses the filters we define for each Convolutional Layer to extract some features from different regions of the image (5x5 regions within the 28x28 images) that it tries to find in other images, irrespective of in which pixels these features are found, because it parses the entire image. Additionally, the Neural Network becomes even more translation-invariant with regard to the extracted features when using the ‘Pooling Layers’, which summarize the values in 2x2 rectangles, so that we can reduce some noise and parameters to estimate, and at the same time being able to detect the same features within such 2x2 rectangles. Finally, a Fully-Connected layer is added to gather all these extracted features and, by properly estimating the weights, yields reliable class predictions to the final softmax layer.

Consequently, this **‘feature-extraction’ property of the Convolutional Neural Networks** makes them very suitable for image prediction, as every image might be different and the features that us, as a humans, would use to determine ‘what an image is about’ may vary a lot from image to image and appear in different  places within the image, which is precisely what CNN exploits to the full.

As final insights, we used other statistical techniques in order to be able to compare results and approaches. We tried with a **Random Forest** with 200 Decision Trees and 8 jobs using the 8 logical cores of the computer (since unlike Logistic Regression, it can deal with more than 2 classes to predict) and we got a Test Accuracy  of 86.13%. It was quite impressing, if we take into account the lower complexity and the lower time it took to get these results. This meant that Random Forests are still a good and precise technique to get reliable predictions with less complexity and resources than Neural Networks.
